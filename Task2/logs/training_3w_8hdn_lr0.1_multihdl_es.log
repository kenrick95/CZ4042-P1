Train on 14447 samples, validate on 3096 samples
Epoch 1/2500
0s - loss: 55519963021.6001 - val_loss: 54605166385.6124
Epoch 2/2500
0s - loss: 53077409289.6042 - val_loss: 51041098283.6589
Epoch 3/2500
0s - loss: 48633993365.2728 - val_loss: 45599799237.7881
Epoch 4/2500
0s - loss: 42544889287.6860 - val_loss: 38763169638.5323
Epoch 5/2500
0s - loss: 35324121440.4142 - val_loss: 31134441186.2326
Epoch 6/2500
0s - loss: 27720168968.8954 - val_loss: 23516195429.8708
Epoch 7/2500
0s - loss: 20409673372.0418 - val_loss: 16669982249.0129
Epoch 8/2500
0s - loss: 14166294555.9975 - val_loss: 11121014903.0698
Epoch 9/2500
0s - loss: 9258881198.5060 - val_loss: 7168171987.0181
Epoch 10/2500
0s - loss: 6266947746.7222 - val_loss: 5344818821.6227
Epoch 11/2500
0s - loss: 5265718220.6476 - val_loss: 5019247066.9561
Epoch 12/2500
0s - loss: 5111467283.8641 - val_loss: 4929786739.7623
Epoch 13/2500
0s - loss: 5052300372.3115 - val_loss: 4868524502.9871
Epoch 14/2500
0s - loss: 5004460527.2724 - val_loss: 4819309056.0000
Epoch 15/2500
0s - loss: 4971269222.7225 - val_loss: 4780457080.3928
Epoch 16/2500
0s - loss: 4944417315.5285 - val_loss: 4748887403.8243
Epoch 17/2500
0s - loss: 4928540760.2808 - val_loss: 4729537616.7028
Epoch 18/2500
0s - loss: 4909462883.6038 - val_loss: 4705502211.9690
Epoch 19/2500
0s - loss: 4895023944.6163 - val_loss: 4693824059.5349
Epoch 20/2500
0s - loss: 4888907576.5443 - val_loss: 4687314762.7494
Epoch 21/2500
0s - loss: 4886469490.2227 - val_loss: 4672679408.1240
Epoch 22/2500
0s - loss: 4881763206.4058 - val_loss: 4665606055.3592
Epoch 23/2500
0s - loss: 4878224120.8589 - val_loss: 4660931525.7881
Epoch 24/2500
0s - loss: 4874560214.6682 - val_loss: 4653278465.9845
Epoch 25/2500
0s - loss: 4872854299.5899 - val_loss: 4651943971.7209
Epoch 26/2500
0s - loss: 4872579379.3170 - val_loss: 4647766559.7519
Epoch 27/2500
0s - loss: 4871739665.9149 - val_loss: 4650752582.1189
Epoch 28/2500
0s - loss: 4866851476.2628 - val_loss: 4641051091.0181
Epoch 29/2500
0s - loss: 4867231523.8829 - val_loss: 4642756560.3721
Epoch 30/2500
0s - loss: 4869721091.9294 - val_loss: 4641019896.0620
Epoch 31/2500
0s - loss: 4869598675.0268 - val_loss: 4640894196.7545
Epoch 32/2500
0s - loss: 4866882951.5044 - val_loss: 4637527882.7494
Epoch 33/2500
0s - loss: 4872486489.9819 - val_loss: 4639480485.3747
Epoch 34/2500
0s - loss: 4868577601.2714 - val_loss: 4634532047.7106
Epoch 35/2500
0s - loss: 4867682883.8674 - val_loss: 4643519675.8656
Epoch 36/2500
0s - loss: 4870120444.2434 - val_loss: 4645044086.4083
Epoch 37/2500
0s - loss: 4874484902.7092 - val_loss: 4638440795.9483
Epoch 38/2500
0s - loss: 4868693547.0240 - val_loss: 4638297192.5168
Epoch 39/2500
0s - loss: 4868683654.3747 - val_loss: 4638727817.5917
Epoch 40/2500
0s - loss: 4867955466.7383 - val_loss: 4639323536.8682
Epoch 41/2500
0s - loss: 4870602153.5621 - val_loss: 4638181190.7804
Epoch 42/2500
0s - loss: 4871452402.0367 - val_loss: 4640356873.2610
Epoch 43/2500
0s - loss: 4868923799.0979 - val_loss: 4634685638.4496
Epoch 44/2500
0s - loss: 4873381986.4166 - val_loss: 4638344431.4625
Epoch 45/2500
0s - loss: 4875392815.6223 - val_loss: 4639173766.9457
----------- Model lr = 0.1, hdn = 8, hdl = 0 
loss: 4998569843.76
Train on 14447 samples, validate on 3096 samples
Epoch 1/2500
0s - loss: 52481288086.6727 - val_loss: 45866886771.1008
Epoch 2/2500
0s - loss: 36189959827.5362 - val_loss: 25136715881.8398
Epoch 3/2500
0s - loss: 16317033211.7295 - val_loss: 8529808958.1809
Epoch 4/2500
0s - loss: 6012077247.1982 - val_loss: 4979645408.2481
Epoch 5/2500
0s - loss: 5040066975.6390 - val_loss: 4804382989.8915
Epoch 6/2500
0s - loss: 4950289827.0943 - val_loss: 4731967876.9612
Epoch 7/2500
0s - loss: 4901490096.7387 - val_loss: 4705676090.8734
Epoch 8/2500
0s - loss: 4885725459.7932 - val_loss: 4664740008.0207
Epoch 9/2500
0s - loss: 4884528824.8190 - val_loss: 4655442389.6641
Epoch 10/2500
0s - loss: 4879362681.2753 - val_loss: 4649021618.6047
Epoch 11/2500
0s - loss: 4879523249.5007 - val_loss: 4638820736.9922
Epoch 12/2500
0s - loss: 4866590499.0235 - val_loss: 4644189013.3333
Epoch 13/2500
0s - loss: 4884198165.2019 - val_loss: 4638607573.0026
Epoch 14/2500
0s - loss: 4882587929.5344 - val_loss: 4648034352.9509
Epoch 15/2500
0s - loss: 4881349287.6661 - val_loss: 4636116737.9845
Epoch 16/2500
0s - loss: 4880463011.7234 - val_loss: 4635723855.3798
Epoch 17/2500
0s - loss: 4874838557.8404 - val_loss: 4651860473.3850
Epoch 18/2500
0s - loss: 4877258430.4539 - val_loss: 4658702395.5349
Epoch 19/2500
0s - loss: 4887596372.5773 - val_loss: 4658101991.5245
Epoch 20/2500
0s - loss: 4878246491.6475 - val_loss: 4645364399.9587
Epoch 21/2500
0s - loss: 4877552639.1494 - val_loss: 4640262678.4910
Epoch 22/2500
0s - loss: 4876114334.9656 - val_loss: 4648329941.0026
Epoch 23/2500
0s - loss: 4876110557.7828 - val_loss: 4644831720.1860
Epoch 24/2500
0s - loss: 4873336593.7199 - val_loss: 4639459194.3773
Epoch 25/2500
0s - loss: 4874643523.3003 - val_loss: 4638875121.4470
Epoch 26/2500
0s - loss: 4861735726.5148 - val_loss: 4659735155.1008
Epoch 27/2500
0s - loss: 4871084835.5994 - val_loss: 4647316257.7364
----------- Model lr = 0.1, hdn = 8, hdl = 1 
loss: 5016824190.35
Train on 14447 samples, validate on 3096 samples
Epoch 1/2500
0s - loss: 12168164843.6221 - val_loss: 4665613624.2274
Epoch 2/2500
0s - loss: 4973018352.1761 - val_loss: 4719291171.0594
Epoch 3/2500
0s - loss: 4969118544.1296 - val_loss: 4661587388.5271
Epoch 4/2500
0s - loss: 4945670878.5625 - val_loss: 4682389713.0336
Epoch 5/2500
0s - loss: 4953523646.6843 - val_loss: 4693022028.0724
Epoch 6/2500
0s - loss: 4938861452.7406 - val_loss: 4677484488.4341
Epoch 7/2500
0s - loss: 4932931993.7028 - val_loss: 4687847865.8811
Epoch 8/2500
0s - loss: 4935045405.9290 - val_loss: 4682243154.0258
Epoch 9/2500
0s - loss: 4929032034.2748 - val_loss: 4792948488.5995
Epoch 10/2500
0s - loss: 4924035970.1884 - val_loss: 4796837559.8966
Epoch 11/2500
0s - loss: 4958575401.3761 - val_loss: 4640512095.2558
Epoch 12/2500
0s - loss: 4936958648.4292 - val_loss: 4760233579.1628
Epoch 13/2500
0s - loss: 4984886236.1703 - val_loss: 4675922213.7054
Epoch 14/2500
0s - loss: 4922442392.8877 - val_loss: 4684416365.1473
Epoch 15/2500
0s - loss: 4934090727.0503 - val_loss: 4849142274.6460
Epoch 16/2500
0s - loss: 4931196228.3458 - val_loss: 4819174401.3230
Epoch 17/2500
0s - loss: 4930787524.4433 - val_loss: 4674851617.7364
Epoch 18/2500
0s - loss: 4941810417.2393 - val_loss: 4932348164.6305
Epoch 19/2500
0s - loss: 4951406369.5793 - val_loss: 4683597710.2222
Epoch 20/2500
0s - loss: 4919295128.5687 - val_loss: 4696142933.9948
Epoch 21/2500
0s - loss: 4929358539.6730 - val_loss: 4758908626.3566
Epoch 22/2500
0s - loss: 4954071293.6078 - val_loss: 4662307485.4367
----------- Model lr = 0.1, hdn = 8, hdl = 2 
loss: 5022170850.23
Train on 14447 samples, validate on 3096 samples
Epoch 1/2500
0s - loss: 7260932654.0896 - val_loss: 4992039379.0181
Epoch 2/2500
0s - loss: 5281930134.3891 - val_loss: 5092937829.8708
Epoch 3/2500
0s - loss: 5205632129.7277 - val_loss: 4868508173.2300
Epoch 4/2500
0s - loss: 5181516950.1233 - val_loss: 5658121454.1395
Epoch 5/2500
0s - loss: 5194332065.8362 - val_loss: 4907354683.5349
Epoch 6/2500
0s - loss: 5132408410.7793 - val_loss: 4652731941.0439
Epoch 7/2500
0s - loss: 5108866938.4271 - val_loss: 5290463351.0698
Epoch 8/2500
0s - loss: 5131282968.4890 - val_loss: 5053629330.1912
Epoch 9/2500
0s - loss: 5090132953.7426 - val_loss: 4753263262.7597
Epoch 10/2500
0s - loss: 5086289130.3462 - val_loss: 5221494543.2145
Epoch 11/2500
0s - loss: 5095192713.9675 - val_loss: 4876725169.9432
Epoch 12/2500
0s - loss: 5078161210.8125 - val_loss: 4673554521.9638
Epoch 13/2500
0s - loss: 5108463168.9259 - val_loss: 4798312171.4935
Epoch 14/2500
0s - loss: 5087361257.3362 - val_loss: 4786768917.1680
Epoch 15/2500
0s - loss: 5063552709.4888 - val_loss: 4786907761.7778
Epoch 16/2500
0s - loss: 5098265159.5886 - val_loss: 4831810460.7752
Epoch 17/2500
0s - loss: 5065636783.6755 - val_loss: 5133598291.3488
----------- Model lr = 0.1, hdn = 8, hdl = 3 
loss: 5369400581.95
Train on 14447 samples, validate on 3096 samples
Epoch 1/2500
1s - loss: 7001456034.2261 - val_loss: 4854604716.6512
Epoch 2/2500
0s - loss: 5944349156.3215 - val_loss: 4965975518.9251
Epoch 3/2500
0s - loss: 5865202747.8934 - val_loss: 5374375644.9406
Epoch 4/2500
0s - loss: 5784187679.0276 - val_loss: 10148554923.9897
Epoch 5/2500
0s - loss: 5704370212.2196 - val_loss: 5550990046.2636
Epoch 6/2500
0s - loss: 5726001006.0940 - val_loss: 6126785987.1421
Epoch 7/2500
0s - loss: 5604514995.7954 - val_loss: 5420622599.2765
Epoch 8/2500
0s - loss: 5532634206.3055 - val_loss: 6352537544.4341
Epoch 9/2500
0s - loss: 5549716923.2643 - val_loss: 4836937352.2687
Epoch 10/2500
0s - loss: 5473321325.1371 - val_loss: 5762967046.6150
Epoch 11/2500
0s - loss: 5411958422.4423 - val_loss: 4903538979.0594
Epoch 12/2500
0s - loss: 5375710201.6208 - val_loss: 7188804319.5866
Epoch 13/2500
0s - loss: 5329333979.4615 - val_loss: 5502989215.4212
Epoch 14/2500
0s - loss: 5251512019.3989 - val_loss: 5943127108.7959
Epoch 15/2500
0s - loss: 5227677157.2695 - val_loss: 5225055315.3488
Epoch 16/2500
0s - loss: 5261856286.3543 - val_loss: 4886834117.7881
Epoch 17/2500
0s - loss: 5168215661.7219 - val_loss: 4912602511.5452
Epoch 18/2500
0s - loss: 5154074482.3822 - val_loss: 5659983557.1266
Epoch 19/2500
0s - loss: 5157738820.8951 - val_loss: 4769241548.4031
Epoch 20/2500
0s - loss: 5092300433.9769 - val_loss: 4964550695.6899
Epoch 21/2500
0s - loss: 5125317359.7508 - val_loss: 4651654678.4910
Epoch 22/2500
0s - loss: 5107584704.1196 - val_loss: 5175079217.6124
Epoch 23/2500
0s - loss: 5087940006.7978 - val_loss: 6929897292.0724
Epoch 24/2500
0s - loss: 5144541944.2209 - val_loss: 4680325217.9018
Epoch 25/2500
0s - loss: 5126100220.5801 - val_loss: 4700193651.7623
Epoch 26/2500
0s - loss: 5122804415.4108 - val_loss: 6855290099.4315
Epoch 27/2500
0s - loss: 5127153807.1328 - val_loss: 4671727795.9276
Epoch 28/2500
0s - loss: 5138604156.6775 - val_loss: 4704163265.8191
Epoch 29/2500
0s - loss: 5099133006.9069 - val_loss: 4715852199.3592
Epoch 30/2500
0s - loss: 5093432284.9854 - val_loss: 5236028904.1860
Epoch 31/2500
0s - loss: 5115084378.6198 - val_loss: 4688168645.1266
Epoch 32/2500
0s - loss: 5131272110.9135 - val_loss: 4672449463.2351
----------- Model lr = 0.1, hdn = 8, hdl = 4 
loss: 5073036350.18
